{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import queue\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def map_reduce(data):\n",
        "\n",
        "  def map_function(document, output_queue):\n",
        "    words = document.lower().split()\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "      if word in word_counts:\n",
        "        word_counts[word] += 1\n",
        "      else:\n",
        "        word_counts[word] = 1\n",
        "    output_queue.put(word_counts)\n",
        "\n",
        "  def shuffle_sort_reduce(input_queue, output_file):\n",
        "    word_counts = {}\n",
        "    while not input_queue.empty():\n",
        "      counts = input_queue.get()\n",
        "      for word, count in counts.items():\n",
        "        if word in word_counts:\n",
        "          word_counts[word] += count\n",
        "        else:\n",
        "          word_counts[word] = count\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow(['Word', 'Count'])\n",
        "      for word, count in sorted(word_counts.items()):\n",
        "        writer.writerow([word, count])\n",
        "\n",
        "  # Create queues\n",
        "  map_queue = queue.Queue()\n",
        "\n",
        "  # Create threads for mapper functions\n",
        "  map_threads = []\n",
        "  for doc in data:\n",
        "    thread = threading.Thread(target=map_function, args=(doc, map_queue))\n",
        "    map_threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "  # Wait for all mapper threads to finish\n",
        "  for thread in map_threads:\n",
        "    thread.join()\n",
        "\n",
        "  # Create shuffle, sort, and reduce thread\n",
        "  shuffle_reduce_thread = threading.Thread(target=shuffle_sort_reduce, args=(map_queue, 'output.csv'))\n",
        "  shuffle_reduce_thread.start()\n",
        "\n",
        "  # Wait for shuffle, sort, and reduce thread to finish\n",
        "  shuffle_reduce_thread.join()\n",
        "\n",
        "  print(\"Word counts written to output.csv\")\n",
        "\n",
        "# Get documents from Google Drive folder\n",
        "data = []\n",
        "folder_path = '/content/data' # Replace with the actual path to your folder\n",
        "for filename in os.listdir(folder_path):\n",
        "  if filename.endswith(\".txt\"):\n",
        "    with open(os.path.join(folder_path, filename), 'r') as f:\n",
        "      data.append(f.read())\n",
        "\n",
        "# Run map reduce\n",
        "map_reduce(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3ChBI4dDFKr",
        "outputId": "94efdd27-43ff-4099-c38f-badd2b350972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word counts written to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: do it for multithreading\n",
        "\n",
        "import threading\n",
        "import queue\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def map_reduce(data):\n",
        "  def map_function(document, output_queue):\n",
        "    words = document.lower().split()\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "      if word in word_counts:\n",
        "        word_counts[word] += 1\n",
        "      else:\n",
        "        word_counts[word] = 1\n",
        "    output_queue.put(word_counts)\n",
        "\n",
        "  def reduce_function(word_counts_chunk, output_queue):\n",
        "    reduced_counts = {}\n",
        "    for word, count in word_counts_chunk.items():\n",
        "      if word in reduced_counts:\n",
        "        reduced_counts[word] += count\n",
        "      else:\n",
        "        reduced_counts[word] = count\n",
        "    output_queue.put(reduced_counts)\n",
        "\n",
        "  def shuffle_sort_reduce(input_queue, output_file):\n",
        "    word_counts = {}\n",
        "    while not input_queue.empty():\n",
        "      counts = input_queue.get()\n",
        "      for word, count in counts.items():\n",
        "        if word in word_counts:\n",
        "          word_counts[word] += count\n",
        "        else:\n",
        "          word_counts[word] = count\n",
        "\n",
        "    # Split word counts into chunks for multiple threads\n",
        "    num_threads = 4  # You can adjust this based on your system\n",
        "    chunk_size = len(word_counts) // num_threads\n",
        "    word_counts_chunks = [dict(list(word_counts.items())[i:i+chunk_size]) for i in range(0, len(word_counts), chunk_size)]\n",
        "\n",
        "    # Create queues and threads for reducers\n",
        "    reduce_queue = queue.Queue()\n",
        "    reduce_threads = []\n",
        "    for chunk in word_counts_chunks:\n",
        "      thread = threading.Thread(target=reduce_function, args=(chunk, reduce_queue))\n",
        "      reduce_threads.append(thread)\n",
        "      thread.start()\n",
        "\n",
        "    # Wait for reducer threads to finish\n",
        "    for thread in reduce_threads:\n",
        "      thread.join()\n",
        "\n",
        "    # Merge results from reducers\n",
        "    final_word_counts = {}\n",
        "    while not reduce_queue.empty():\n",
        "      counts = reduce_queue.get()\n",
        "      for word, count in counts.items():\n",
        "        if word in final_word_counts:\n",
        "          final_word_counts[word] += count\n",
        "        else:\n",
        "          final_word_counts[word] = count\n",
        "\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow(['Word', 'Count'])\n",
        "      for word, count in sorted(final_word_counts.items()):\n",
        "        writer.writerow([word, count])\n",
        "\n",
        "  # Create queues\n",
        "  map_queue = queue.Queue()\n",
        "\n",
        "  # Create threads for mapper functions\n",
        "  map_threads = []\n",
        "  for doc in data:\n",
        "    thread = threading.Thread(target=map_function, args=(doc, map_queue))\n",
        "    map_threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "  # Wait for all mapper threads to finish\n",
        "  for thread in map_threads:\n",
        "    thread.join()\n",
        "\n",
        "  # Create shuffle, sort, and reduce thread\n",
        "  shuffle_reduce_thread = threading.Thread(target=shuffle_sort_reduce, args=(map_queue, 'output2.csv'))\n",
        "  shuffle_reduce_thread.start()\n",
        "\n",
        "  # Wait for shuffle, sort, and reduce thread to finish\n",
        "  shuffle_reduce_thread.join()\n",
        "\n",
        "  print(\"Word counts written to output.csv\")\n",
        "\n",
        "# Get documents from Google Drive folder\n",
        "data = []\n",
        "folder_path = '/content/data' # Replace with the actual path to your folder\n",
        "for filename in os.listdir(folder_path):\n",
        "  if filename.endswith(\".txt\"):\n",
        "    with open(os.path.join(folder_path, filename), 'r') as f:\n",
        "      data.append(f.read())\n",
        "\n",
        "# Run map reduce\n",
        "map_reduce(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPGcx1HnGRBz",
        "outputId": "ffe134e8-f90c-438f-9a8b-8c4f3a170ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word counts written to output.csv\n"
          ]
        }
      ]
    }
  ]
}